n_layers: 2
n_heads: 2
hidden_size: 300
inner_size: 256
hidden_dropout_prob: 0.2
attn_dropout_prob: 0.2
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: 'CE'

item_drop_ratio: 0.2
item_drop_coefficient: 0.5
lambda: 1e-3

plm_suffix: feat1CLS
#plm_suffix: pth
gpu_id: 2
plm_suffix_aug: None
train_stage: inductive_ft  # pretrain / inductive_ft / transductive_ft
plm_size: 768
adaptor_dropout_prob: 0.2
adaptor_layers: [768, 300]
temperature: 0.07
n_exps: 8

eval_args: {split: ['LS': 'valid_and_test'],
            order: 'TO',
            group_by: 'user',
            mode: 'pop100'
          }
